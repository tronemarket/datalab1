{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting BQ Tables to Google Storage\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "print ('Start of the code')\n",
    "bigquery_client = bigquery.Client()\n",
    "dataset_ref = bigquery_client.dataset(\"tom\") # DATASET OF BQ\n",
    "table_ref = dataset_ref.table(\"Yocuda_clean_data_Nov15_Nov17_20171214_v01\") # TABLE NAME OF BQ\n",
    "\n",
    "\n",
    "job = bigquery_client.extract_table(table_ref, 'gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv') # GOOGLE CLOUD BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing \n",
    "\n",
    "data = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          identifier|\n",
      "+--------------------+\n",
      "|mail@jonmorris.co.uk|\n",
      "|zara.bett@icloud.com|\n",
      "|sjones31@btintern...|\n",
      "|  kbach1@yahoo.co.uk|\n",
      "|johnbaillie77@hot...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "overall = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & (to_date(col(\"timestamp\"))<'2017-12-01') & (col(\"retailer_name\")=='Argos')).select(\"identifier\").distinct()\n",
    "overall.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier|    brand_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|                0.0|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com|                0.0|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11224605"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brand\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"brandName\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "brand = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"brand_diversity\"))\n",
    "\n",
    "brand.show(5)\n",
    "brand.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier| category_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|  0.299594933046483|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com| 0.2863175935284513|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11224605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"cat2\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "category = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"category_diversity\"))\n",
    "\n",
    "category.show(5)\n",
    "category.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          identifier|          timeseries|\n",
      "+--------------------+--------------------+\n",
      "|michaelrees49@goo...|  0.6304815072659585|\n",
      "|taymar1946@hotmai...|  1.7849737099544007|\n",
      "|  i.livesley@sky.com| 0.07806447558855253|\n",
      "|janedayisjustjane...|0.024625816666481892|\n",
      "| l344@btinternet.com|  0.7194326750721033|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12103974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.registerTempTable(\"data\")\n",
    "\n",
    "xx1 =data.selectExpr(\"identifier\",\"month(timestamp) as month\", \"year(timestamp) as year\", \"item_total\", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\n",
    "\n",
    "xx2 =data.selectExpr(\"timestamp \", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(col(\"ind\")).agg(max(\"timestamp\").alias(\"timestamp_mx\"))\n",
    "\n",
    "xx3 = xx1.join(xx2,[\"ind\"],\"left\")\n",
    "\n",
    "xx4 = xx3.selectExpr(\"identifier\",\"((month(timestamp_mx) - month)+ (year(timestamp_mx)- year)) as recency\", \"item_total\")\\\n",
    ".groupBy(col(\"identifier\"),col(\"recency\")).agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "timeseries = xx4.selectExpr(\"identifier\", \"((exp((-1) * recency)) * log10(spend)) as timeseries\")\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"timeseries\").alias(\"timeseries\"))\n",
    "\n",
    "timeseries.show(5)\n",
    "timeseries.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-933f173b04ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "combined = overall.join(brand,[\"identifier\"],\"left\").join(category,[\"identifier\"],\"left\").join(timeseries,[\"identifier\"],\"left\")\n",
    "\n",
    "combined.show(5)\n",
    "combined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data and starting the process\n",
    "data_new = combined\n",
    "\n",
    "# Replacing the missing values by 0 \n",
    "# Although it's not necessary as there are no missing values most of the time\n",
    "data_new = data_new.fillna(0)\n",
    "\n",
    "# Creating spark sql tables for faster extraction of the percentiles\n",
    "data_new.createOrReplaceTempView('temporary_intermediate_table')\n",
    "\n",
    "# Storing the column names in a set (ordered list of sorts)\n",
    "col_names = set(data_new.columns) - {\"identifier\"}\n",
    "\n",
    "# Writing queries to calculate the 99th (upper limit) and 1st (lower limit) percentile\n",
    "upper_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.99) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "lower_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.01) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Saving the capping limits in two pandas DataFrames \n",
    "upper_limit = spark.sql(upper_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "lower_limit = spark.sql(lower_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "\n",
    "# Deciding which columns need outlier treatment (Dropping those which have 1st and 99th percentile as same)\n",
    "columns_to_treat = {var_name for var_name in col_names if upper_limit.iloc[0][var_name] > lower_limit.iloc[0][var_name]}\n",
    "\n",
    "# Writing a SQL query for outlier treatment\n",
    "outlier_treated_data = spark.sql('SELECT identifier, '+ ', '.join(['IF({var_name} >= {maximum}, {maximum}, IF({var_name} <={minimum}, {minimum}, {var_name})) AS  {var_name}'.format(var_name = var_name, maximum = upper_limit.iloc[0][var_name], minimum = lower_limit.iloc[0][var_name])  for var_name in columns_to_treat]) + ' FROM ' + 'temporary_intermediate_table')\n",
    "\n",
    "outlier_treated_data.show(5)\n",
    "outlier_treated_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Defining the function to normalize the data.\n",
    "def normalization_missing_value(data):\n",
    "  i = 0\n",
    "#   Transforming the variables as per Maximum and Minimum Value\n",
    "  for cols in data.columns[1:]:\n",
    "    maxs = data.agg(max(col(cols)).alias(\"max\")).toPandas()\n",
    "    mins = data.agg(min(col(cols)).alias(\"min\")).toPandas()\n",
    "    data = data.withColumn(cols,((col(cols)-mins[\"min\"][0])/(maxs[\"max\"][0]-mins[\"min\"][0])))\n",
    "    i = i+1\n",
    "  return data\n",
    "                           \n",
    "final= normalization_missing_value(outlier_treated_data)\n",
    "final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "df3 = df2.withColumn(\"bucket\", (when(col(\"percentile\")<=0.10,\"0-10\").otherwise(\\\n",
    "      when(col('percentile')<=0.20,\"10-20\").otherwise(\\\n",
    "        when(col('percentile')<=0.30,\"20-30\").otherwise(\\\n",
    "          when(col('percentile')<=0.40,\"30-40\").otherwise(\\\n",
    "            when(col('percentile')<=0.50,\"40-50\").otherwise(\\\n",
    "              when(col('percentile')<=0.60,\"50-60\").otherwise(\\\n",
    "                when(col('percentile')<=0.70,\"60-70\").otherwise(\\\n",
    "                  when(col('percentile')<=0.80,\"70-80\").otherwise(\\\n",
    "                    when(col('percentile')<=0.90,\"80-90\").otherwise('90-100')))))))))))\\\n",
    "\n",
    "output = df3.groupBy(\"bucket\").agg((sum(col(\"spend\")).alias(\"spend\"))\\\n",
    "                       ,(countDistinct(col(\"identifier\")).alias(\"cnt_shp\"))\\\n",
    "                       ,(avg(col(\"spend\")).alias(\"avg_spend\")))\n",
    "\n",
    "output.show()\n",
    "\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "target = df2.withColumn(\"target\",(when(col(\"percentile\")<=0.20,1).otherwise(0)))\\\n",
    "                     .select('identifier','target')\n",
    "\n",
    "final_w_target = final.join(target,[\"identifier\"],\"left\").fillna(0)\n",
    "\n",
    "final_w_target.show(5)\n",
    "\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "random.seed(7)\n",
    "train, test = final_w_target.randomSplit([0.8, 0.2])\n",
    "\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Saving Test and train dataset ############\n",
    "\n",
    "train.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/train_set2\")\n",
    "test.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/test_set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
