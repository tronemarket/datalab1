
import pandas as pd
​
from google.cloud import bigquery
​
from pyspark.sql.functions import *
from pyspark.sql.functions import col,udf,array
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vectors, VectorUDT
​
from pyspark.sql.functions import col,udf,array
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vectors, VectorUDT

# Exporting BQ Tables to Google Storage
​
from google.cloud import bigquery
​
print ('Start of the code')
bigquery_client = bigquery.Client()
dataset_ref = bigquery_client.dataset("tom") # DATASET OF BQ
table_ref = dataset_ref.table("Yocuda_clean_data_Nov15_Nov17_20171214_v01") # TABLE NAME OF BQ
​
​
job = bigquery_client.extract_table(table_ref, 'gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv') # GOOGLE CLOUD BUCKET
